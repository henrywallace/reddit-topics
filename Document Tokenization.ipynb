{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I'll just open up a text-heavy subreddit such as r/todayilearned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('docs/todayilearned[10-3].p', 'rb') as f:\n",
    "    _, docs, trans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>I saw one boy daydreaming and just swinging his broom in the air. \\n\\n>Me (In Japanese): \"What the hell are you doing?\" \\n\\n>**\"Uh, you know... air pollution.\"**\\n\\nGive that kid a medal.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this document is pretty heavy with Markdown markup. After some looking around, apparently the easiest stripping of this markup is to simply convert it to HTML and then strip the HTML. We accomplish this with `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI saw one boy daydreaming and just swinging his broom in the air. \\nMe (In Japanese): \"What the hell are you doing?\" \\n\"Uh, you know... air pollution.\"\\n\\nGive that kid a medal.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = markdown(docs[2])\n",
    "text = ''.join(BeautifulSoup(html, 'html.parser').findAll(text=True))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we need to tokenize the text into words. I prefer regex tokenization, since contractions and punctuation makes things hairy. And we'll lowercase everything. Also, I'll include subreddit names and dash name combinations as part of the regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'saw', 'one', 'boy', 'daydreaming', 'and', 'just', 'swinging', 'his', 'broom', 'in', 'the', 'air', 'me', 'in', 'japanese', 'what', 'the', 'hell', 'are', 'you', 'doing', 'uh', 'you', 'know', 'air', 'pollution', 'give', 'that', 'kid', 'a', 'medal']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"([\\w'-]+)|/r/\\w+\")\n",
    "print(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's encapsulate this into a nice little function. We'll feed it a list of raw untokenized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_tokenize(docs):\n",
    "    tokenizer = RegexpTokenizer(\"([\\w'-]+)|/r/\\w+\")\n",
    "    for d in docs:\n",
    "        html = markdown(d)\n",
    "        text = ''.join(BeautifulSoup(html, 'html.parser').findAll(text=True))\n",
    "        yield tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since that was so quick let's start on the LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda_gibbs import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_docs = list(docs_tokenize(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LDA(token_docs)\n",
    "theta, beta = model.train(ntopics=20, niter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 ('http', 'com', 'www', 'news', 'watch', '10', 'he', 'https', 'uk', 'source')\n",
      "0.362230566279 ('be', 'you', 'would', 'why', 'it', 'if', 'do', 'is', \"that's\", 'not')\n",
      "0.279827290821 ('i', 'you', 'that', 'a', 'know', 'about', 'me', 'am', 'the', 'for')\n",
      "0.264362053181 ('money', 'we', 'for', 'are', 'have', 'on', 'all', 'back', 'still', 'he')\n",
      "0.261539971002 ('blood', 'his', 'he', 'donate', 'have', 'this', 'antigen', 'maybe', 'guy', 'from')\n",
      "0.212612546494 ('in', '2', '1', 'about', '000', 'of', 'years', 'one', 'the', 'world')\n",
      "0.208819559615 ('beard', 'you', 'grow', 'i', 'get', 'have', 'it', 'to', 'would', 'with')\n",
      "0.204588527388 ('they', 'i', 'know', 'what', 'he', 'that', 'think', 'even', 'a', 'could')\n",
      "0.200204152086 ('the', 'my', 'in', 'i', 'a', 'one', 'never', 'no', 'good', 'this')\n",
      "0.182960059965 ('like', 'i', 'me', 'a', 'you', 'mean', 'but', \"it's\", 'this', 'go')\n",
      "0.166761772088 ('was', 'in', 'had', 'i', 'my', 'and', 'a', 'were', 'one', 'school')\n",
      "0.163292645184 ('not', 'so', 'that', 'is', 'with', 'something', 'he', 'out', 'him', 'an')\n",
      "0.107911869374 ('you', 'that', 'is', 'not', 'i', \"don't\", 'so', 'make', 'of', 'really')\n",
      "0.0923371599544 ('the', 'people', 'that', 'with', 'horn', 'will', 'rhinos', 'rhino', 'poachers', 'there')\n",
      "0.0873809725603 ('more', 'than', \"it's\", 'them', 'be', 'would', 'most', 'and', 'give', 'making')\n",
      "0.0783743499402 ('you', 'your', 'it', 'if', 'out', 'they', 'game', 'get', 'with', 'money')\n",
      "0.0751460160557 ('not', 'any', 'because', 'just', 'if', 'in', 'sure', 'on', 'does', \"i'm\")\n",
      "0.0617451840139 ('clean', 'in', 'kids', 'at', 'they', 'but', 'their', 'i', 'up', 'to')\n",
      "0.0158422548911 ('do', 'have', 'down', 'they', 'their', 'but', 'if', 'of', 'dog', 'got')\n",
      "0.0 ('are', 'people', 'that', 'they', 'of', 'for', 'be', 'very', 'the', 'other')\n"
     ]
    }
   ],
   "source": [
    "tsr = model.topic_significances()\n",
    "named_tsr = [(s, model.topic_representatives(i, topn=10, show_scores=False)) for i, s in enumerate(tsr)]\n",
    "for name, score in sorted(named_tsr, reverse=True):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
